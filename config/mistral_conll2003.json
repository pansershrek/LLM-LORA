{
    "MODEL_NAME": "mistralai/Mistral-7B-Instruct-v0.1",
    "MODEL_OUTPUT": "/data/LLM-LORA/result/mistral_finetune_5",
    "MERGED_MODEL_PATH": "/data/LLM-LORA/weights_merged",
    "TRAIN_DATASET": "/data/LLM-LORA/dataset/conll2003_dataset_train.json",
    "TEST_DATASET": "/data/LLM-LORA/dataset/conll2003_dataset_test.json",
    "DATASET": "conll2003",
    "TRAIN_PARAMS": {
        "MICRO_BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,
        "EPOCHS": 3,
        "LEARNING_RATE": 3e-4,
        "MAX_LEN": 1152,
        "DEVICE_MAP": "auto",
        "LOAD_IN_8BIT": true,
        "USE_FLASH_ATTENTION_2": true,
        "WARMUP_STEPS": 100,
        "REPORT_TO": "wandb",
        "FP16": true
    },
    "LORA_PARAMS": {
        "LORA_R": 4,
        "LORA_ALPHA": 8,
        "LORA_DROPOUT": 0.05,
        "TARGET_MODULES": [
            "q_proj", "k_proj", "v_proj", "o_proj"
        ]
    },
    "SAMPLING_PARAMS": {
        "TEMPERATURE": 1,
        "TOP_K": 50,
        "TOP_P": 1.0,
        "MAX_TOKENS": 1152,
        "BATCH_SIZE": 4
    }
}