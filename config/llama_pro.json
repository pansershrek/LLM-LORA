{
    "MODEL_NAME": "TencentARC/LLaMA-Pro-8B",
    "MODEL_OUTPUT": "/mnt/g.skiba/LLM-LORA/result/llama2_pro",
    "MERGED_MODEL_PATH": "/mnt/g.skiba/LLM-LORA/llama_pro_weights_merged",
    "TRAIN_DATASET": "/mnt/g.skiba/LLM-LORA/dataset/conll2003_dataset_train.json",
    "TEST_DATASET": "/mnt/g.skiba/LLM-LORA/dataset/conll2003_dataset_test.json",
    "DATASET": "conll2003",
    "LLAMA_PRO": true,
    "TRAIN_PARAMS": {
        "MICRO_BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,
        "EPOCHS": 2,
        "LEARNING_RATE": 1e-5,
        "MAX_LEN": 1262,
        "DEVICE_MAP": "auto",
        "LOAD_IN_8BIT": false,
        "USE_FLASH_ATTENTION_2": true,
        "WARMUP_STEPS": 10,
        "REPORT_TO": "wandb",
        "FP16": true
    },
    "SAMPLING_PARAMS": {
        "TEMPERATURE": 1,
        "TOP_K": 50,
        "TOP_P": 1.0,
        "MAX_TOKENS": 1262,
        "BATCH_SIZE": 512
    },
    "SEED": 1979
}