{
    "MODEL_NAME": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "MODEL_OUTPUT": "/data/LLM-LORA/result/mixtral_1",
    "MERGED_MODEL_PATH": "/data/LLM-LORA/mixtral_weights_merged_1",
    "TRAIN_DATASET": "/data/LLM-LORA/dataset/conll2003_dataset_train.json",
    "TEST_DATASET": "/data/LLM-LORA/dataset/conll2003_dataset_test.json",
    "DATASET": "conll2003",
    "ONLY_EVAL": true,
    "TRAIN_PARAMS": {
        "MICRO_BATCH_SIZE": 4,
        "GRADIENT_ACCUMULATION_STEPS": 4,
        "EPOCHS": 1,
        "LEARNING_RATE": 3e-4,
        "MAX_LEN": 1262,
        "DEVICE_MAP": "auto",
        "LOAD_IN_8BIT": true,
        "USE_FLASH_ATTENTION_2": true,
        "WARMUP_STEPS": 10,
        "REPORT_TO": "wandb",
        "FP16": true
    },
    "LORA_PARAMS": {
        "LORA_R": 4,
        "LORA_ALPHA": 8,
        "LORA_DROPOUT": 0.05,
        "TARGET_MODULES": [
            "q_proj", "k_proj", "v_proj", "o_proj"
        ]
    },
    "SAMPLING_PARAMS": {
        "TEMPERATURE": 1,
        "TOP_K": 50,
        "TOP_P": 1.0,
        "MAX_TOKENS": 1262,
        "BATCH_SIZE": 512
    }
}